---
category: 
  - data
section: Introduction
title: Introduction
description: Introduction to the Data Science documents
slug: intro
path: data/
---

import { Blockquote, Definition, Note } from '$styles';
import { colors } from 'gatsby-theme-apollo-core';

## What is data science?  
<h4 style={{
  color: colors.primary
}}>
Data science is a tool that solves certain problems.</h4>

There's a real temptation to think of data science as a magical cure-all—as in, "If we could just data science real good, we'd win."

It solves some problems better, faster and more cost-effectively than other tools and may be the only tool that can solve particular problems, but, from a product manager's perspective, the most important thing to remember about data science is that it is a tool that solves certain problems. 

Like other tools, to make effective use of data science, it must be applied to the correct problem. Most data scientists will readily admit that, particularly in a commercial context, identifying and clearly defining the problem to be solved and setting metrics for measuring acceptable results is more difficult and time-consuming than developing an effective model.

<Note>
Data science is relatively new and evolving quickly. The taxonomy and terms of art are unsettled. Universities are rapidly creating data science degrees to meet industry demand for skilled data scientists. Some schools have chosen to add data science curriculum to their statistics departments. Others have added it to their computer science schools. If an article or book classifies something or uses a term in a way that is slightly different than you've come to expect, it probably isn't you.
</Note>
<br/>
<p>Despite the note above, the diagram below is widely accepted and useful.</p>
<br/>

![ai-ml-dl](./images/ai-ml-dl.png)

<br/>
<br/>
<Definition>
  <strong>Artificial Intelligence:</strong> A program that can sense, reason, act and adapt. <br/>  
  <strong>Machine Learning:</strong> Algorithms whose performance improve as they are exposed to more data over time.<br/>
  <strong>Deep Learning:</strong> Subset of machine learning in which multilayered neural networks learn from vast amounts of data.
</Definition>

Today, data science is primarily conducted within the Machine Learning sphere in the diagram above. Before we dive fully into that Machine Learning sphere for the remainder of this site, it's worth making a note of [OpenAI](https://openai.com/). OpenAI's mission "is to ensure that artificial general intelligence (AGI) benefits all of humanity, primarily by attempting to build safe AGI and share the benefits with the world." OpenAI was founded by Elon Musk, Sam Altman and other prominent Silicon Valley dwellers. Musk left the board in early 2018.  

Unless otherwise noted, this site will deal only with topics that fit wholly within the machine learning sphere, and its subset, deep learning.

## How does it work?  
<h4 style={{
  color: colors.primary,
}}>
Machine learning is a tool that builds a model to transform input data into desired output.</h4>

The goal of machine learning is to derive a mathematical formula, a model, that will take data provided to it as input and transform it into output. 

"Machine Learning" is poorly named; there is no learning. When an animal learns something, it's able to use what it has learned to notice if and when circumstances change and how to change its behavior in response. That is not the case with a machine learning model. The model is created to fit very precisely to the data that was used to create it. If it encounters data that differs from the data used to create the model, it cannot adjust. Its performance will simply degrade. Digging a little deeper into how machine learning models are created will reveal why this is. Let's take an email spam filter as an example. 

### 1. Define the problem to be solved
In this case, the problem is clear and doesn't require much exploration. Users' inboxes are being flooded with unwanted emails. We can make our users happier by accurately diverting spam email into a junk folder, thus alleviating the burden of wading through unwanted email. Filtering spam from ham (i.e., not spam) is a binary classification problem. Emails are the input, the spam filtering rules are the algorithm to be applied and the result is one of two things–either ham or spam.  

### 2. Set success metrics
We set measurable goals in advance so we all know what we're working towards and we'll know when we've reached our goal. In this case, we've agreed upon the following:
- fewer than 5% of the email that makes it to a user's inbox is spam, and  
- fewer than 5% of _ham_ is wrongly diverted to the junk folder.

We've decided to make this a competition between Oscar, our object-oriented programming guru, and Melissa, our machine learning intern.

If a programmer were to create a spam filter using a traditional software approach, he might begin by looking at the body of the email for words or phrases that are common to spam emails and add rules to mark emails containing these words and phrases as spam. The creator could then move on to the email metadata and add rules for email addresses that frequently send spam and subject lines containing words like "special offer". You can imagine that the number of rules is becoming increasingly unwieldy and brittle by this point. Now, spam senders are catching on to the fact that their emails are no longer getting through. In order to get their spam past this filter, they start using new email addresses and their subject lines now contain "s p e c i a l   o f f e r", instead of "special offer". The diligent spam filter creator gets back to work, adding new rules for each new variant.

Machine learning differs. Let's look at how a data scientist would approach this binary classification problem. 

### 3. Prepare & explore the data
The data scientist would start by collecting, say, 50,000 emails that have been identified and marked as either "spam" or "ham", hopefully, in roughly even amounts. She would start by dividing the emails as follows:  
1. 10,000 in a _test set_ (generally, 20-30% of all data),  
2. 10,000 in a _validation set_ (20-30% of the data left after the test set has been taken), and  
3. the remaining 30,000 will be used to train the model (the _training set_).  


After separating the data, she would perform exploratory data analysis ("EDA"). During EDA, she'd look at characteristics of the data set
 - similarities among the email marked as spam,  
 - similarities among emails marked as ham, and  
 - differences between the two sets.  
 
The algorithm would then create a model that embodied its findings. The data scientist would feed the remaining emails through the model created and look at performance metrics, such as:  
  
  1. How many ham emails were correctly marked ham?  
  2. How many spam emails were mistakenly marked ham?  
  3. How many spam emails were correctly marked spam?  
  4. How many ham emails were mistakenly marked spam?  
  
She would likely repeat these steps with additional algorithms, looking for the best out-of-the-box performance. If any of the algorithms produced models whose performance approached her pre-established goals, she might choose one or two of them for further refinements. Finally, she would test the refined model or models on the 10,000 emails she had initially set aside for final testing. The model's performance on this final set of previously unseen emails is the best measure of its ability to filter spam from ham. It's important to note that once her filter is in production, she must continue to monitor its performance from time-to-time. Models tend to "drift", or lose accuracy over time. In this instance, you can imagine that spam senders will be looking for new ways to thwart the filter and the filter will need to be updated occasionally to account for these new tactics.
  
The most important takeaways:
- the data scientist did not write any rules herself; the algorithm generated rules after being given labeled data (emails marked as ham or spam)
- the problem to be solved was clearly defined in advance  
- the metrics required for a proposed solution to be deemed a success were clearly defined in advance  
- the emails were generally representative of future emails that the model would be asked to mark as spam or ham  
- upon receipt of the full data set, the data scientist immediately set aside 10,000 of the emails for final testing (the "test set", generally 20-30% of the total)   
  - of the remaining 40,000 emails, 30,000 were used to build models (the "training set")  
  - and the other 10,000 emails were used to validate the models (the "validation set")  

## Why would I use it?  