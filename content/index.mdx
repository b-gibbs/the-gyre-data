---
category: 
  - data
section: Introduction
title: Introduction
description: Introduction to the Data Science documents
slug: intro
path: data/
---

import { Blockquote, Definition, Note } from '$styles';
import { colors } from 'gatsby-theme-apollo-core';

## What is data science?  
<h4 style={{
  color: colors.primary
}}>
Data science is a tool that solves certain problems.</h4>

There's a real temptation to think of data science as a magical cure-all—as in, "If we could just data science real good, we'd win."

It solves some problems better, faster and more cost-effectively than other tools and may be the only tool that can solve particular problems, but, from a product manager's perspective, the first and most important thing to know about data science is that it is a tool that solves certain problems. It should be employed as such.

Like other tools, to make effective use of data science, it must be applied to the correct problem. Most data scientists will readily admit that, particularly in a commercial context, identifying and clearly defining the problem to be solved and setting metrics for measuring acceptable results is much more difficult and more time-consuming than developing an effective model.

<Note>
Data science is relatively new and evolving quickly. The taxonomy and terms of art are unsettled. Universities are rapidly creating data science degrees to meet industry demand for skilled data scientists. Some schools have chosen to add data science curriculum to their statistics departments. Others have added it to their computer science schools. If an article or book classifies something or uses a term in a way that is slightly different than you've come to expect, it probably isn't you.
</Note>
<br/>
<p>Despite the note above, the diagram below is widely accepted and useful.</p>
<br/>

![ai-ml-dl](./images/ai-ml-dl.png)

<br/>
<br/>
<Definition>
  <strong>Artificial Intelligence:</strong> A program that can sense, reason, act and adapt. <br/>  
  <strong>Machine Learning:</strong> Algorithms whose performance improve as they are exposed to more data over time.<br/>
  <strong>Deep Learning:</strong> Subset of machine learning in which multilayered neural networks learn from vast amounts of data.
</Definition>

Today, data science is primarily conducted within the Machine Learning sphere in the diagram above. Before we dive fully into that Machine Learning sphere for the remainder of this site, it's worth making a note of [OpenAI](https://openai.com/). OpenAI's mission "is to ensure that artificial general intelligence (AGI) benefits all of humanity, primarily by attempting to build safe AGI and share the benefits with the world." OpenAI was founded by Elon Musk, Sam Altman and other prominent Silicon Valley dwellers. Musk left the board in early 2018.  

Unless otherwise noted, this site will deal only with topics that fit wholly within the machine learning sphere, and its subset, deep learning.

## How does it work?  
<h4 style={{
  color: colors.primary,
}}>
Machine learning is a tool that creates a formula to transform input data into desired output.</h4>

The goal of machine learning is to derive a mathematical formula that will take data provided to it as input and transform it into output. 

"Machine Learning" is poorly named. There is no machine. There's a mathematical formula processing data, running on some form of compute power. And, there is no learning. When an animal learns something, it notices when circumstances change and will adjust accodingly, based on its understanding of what it has learned. That is not the case with a machine learning model. The model is created to fit very precisely to the data that was used to create it. If the formula encounters data that differs from the data used to create the formula, it cannot adjust. Its performance will simply degrade. Digging a little deeper into how machine learning models are created will reveal why this is.

Let's take an email spam filter as an example. The problem is clear: users' inboxes are being flooded with unwanted emails. We can make our users happier by accurately diverting spam email into a junk folder, thus alleviating the burden of wading through unwanted email. Filtering spam from ham (fun fact: "ham" is the term used to describe email that is not spam), is a binary classification problem. Emails are the input, the spam filtering rules are the algorithm to be applied and the result is one of two things–either ham or spam.  

Let's measure success as:  
- fewer than 5% of the email that makes it to a user's inbox is spam, and  
- fewer than 5% of ham is wrongly diverted to the junk folder.

If a programmer were to create a spam filter using a traditional software approach, he would begin by looking at the body of the email for words or phrases that are common to spam emails and add rules to mark emails containing these words and phrases as spam. The creator could then move on to the email metadata and add rules for email addresses that frequently send spam and subject lines containing words like "special offer". You can imagine that the number of rules is becoming increasingly unwieldy and brittle by this point. Now, spam senders are catching on to the fact that their emails are no longer getting through. In order to get their spam past this filter, they start using new email addresses and their subject lines now contain "s p e c i a l   o f f e r", instead of "special offer". The diligent spam filter creator gets back to work, adding new rules for each new variant.

Machine learning differs. Let's look at how a data scientist would approach this binary classification problem. 

The data scientist would start by collecting, say, 50,000 emails that have been identified and marked as either "spam" or "ham", hopefully, in roughly even amounts. She would start by setting aside 10,000 of the emails for final testing. She would then feed a portion of the remaining emails through an algorithm that would look for: 
 - similarities among the email marked as spam,  
 - similarities among emails marked as ham, and  
 - differences between the two sets.  
 
The algorithm would then create a model that embodied its findings. The data scientist would feed the remaining emails through the model created and look at performance metrics, such as:  
  
  1. How many ham emails were correctly marked ham?  
  2. How many spam emails were mistakenly marked ham?  
  3. How many spam emails were correctly marked spam?  
  4. How many ham emails were mistakenly marked spam?  
  
She would likely repeat these steps with additional algolrithms, looking for the best out-of-the-box performance. If any of the algorithms produced models whose performance approached her pre-established goals, she might choose one or two of them for further refinements. Finally, she would test the refined model or models on the 10,000 emails she had initally set aside for final testing. The model's performance on this final set of previously unseen emails is the best measure of its ability to filter spam from ham. It's important to note that once her filter is in production, she must continue to monitor its performance from time-to-time. Models tend to "drift", or lose accuracy over time. In this instance, you can imagine that spam senders will be looking for new ways to thwart the filter and the filter will need to be updated occasionally to account for these new tactics.
  
The most important takeaways:
- the data scientist did not write any rules herself; the algorithm generated rules after being given labeled data (emails marked as ham or spam  )
- the problem to be solved was clearly defined in advance  
- the metrics required for a proposed solution to be deemed a success were clearly defined in advance  
- the emails were generally representative of future emails that the model would be asked to mark as spam or ham  
- upon receipt of the full data set, the data scientist immediately set aside 10,000 of the emails for final tesing (the "test set", generally 20-30% of the total)   
  - of the remaining 40,000 emails, 30,000 were used to build models (the "training set")  
  - and the other 10,000 emails were used to validate the models (the "validation set")  

## Why would I use it?  